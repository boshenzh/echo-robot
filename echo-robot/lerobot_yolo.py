#!/usr/bin/env python3
"""
ç®€åŒ–çš„é”®ç›˜æ§åˆ¶SO100/SO101æœºå™¨äºº
ä¿®å¤äº†åŠ¨ä½œæ ¼å¼è½¬æ¢é—®é¢˜
ä½¿ç”¨Pæ§åˆ¶ï¼Œé”®ç›˜åªæ”¹å˜ç›®æ ‡å…³èŠ‚è§’åº¦
"""

import time
import logging
import traceback
import math
import cv2
import serial
import threading
from ultralytics import YOLO

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å…³èŠ‚æ ¡å‡†ç³»æ•° - æ‰‹åŠ¨ç¼–è¾‘
# æ ¼å¼: [å…³èŠ‚åç§°, é›¶ä½ç½®åç§»(åº¦), ç¼©æ”¾ç³»æ•°]
JOINT_CALIBRATION = [
    ['shoulder_pan', 6.0, 1.0],      # å…³èŠ‚1: é›¶ä½ç½®åç§», ç¼©æ”¾ç³»æ•°
    ['shoulder_lift', 2.0, 0.97],     # å…³èŠ‚2: é›¶ä½ç½®åç§», ç¼©æ”¾ç³»æ•°
    ['elbow_flex', 0.0, 1.05],        # å…³èŠ‚3: é›¶ä½ç½®åç§», ç¼©æ”¾ç³»æ•°
    ['wrist_flex', 0.0, 0.94],        # å…³èŠ‚4: é›¶ä½ç½®åç§», ç¼©æ”¾ç³»æ•°
    ['wrist_roll', 0.0, 0.5],        # å…³èŠ‚5: é›¶ä½ç½®åç§», ç¼©æ”¾ç³»æ•°
    ['gripper', 0.0, 1.0],           # å…³èŠ‚6: é›¶ä½ç½®åç§», ç¼©æ”¾ç³»æ•°
]

def apply_joint_calibration(joint_name, raw_position):
    """
    åº”ç”¨å…³èŠ‚æ ¡å‡†ç³»æ•°
    
    Args:
        joint_name: å…³èŠ‚åç§°
        raw_position: åŸå§‹ä½ç½®å€¼
    
    Returns:
        calibrated_position: æ ¡å‡†åçš„ä½ç½®å€¼
    """
    for joint_cal in JOINT_CALIBRATION:
        if joint_cal[0] == joint_name:
            offset = joint_cal[1]  # é›¶ä½ç½®åç§»
            scale = joint_cal[2]   # ç¼©æ”¾ç³»æ•°
            calibrated_position = (raw_position - offset) * scale
            return calibrated_position
    return raw_position  # å¦‚æœæ²¡æ‰¾åˆ°æ ¡å‡†ç³»æ•°ï¼Œè¿”å›åŸå§‹å€¼

def inverse_kinematics(x, y, l1=0.1159, l2=0.1350):
    """
    Calculate inverse kinematics for a 2-link robotic arm, considering joint offsets
    
    Parameters:
        x: End effector x coordinate
        y: End effector y coordinate
        l1: Upper arm length (default 0.1159 m)
        l2: Lower arm length (default 0.1350 m)
        
    Returns:
        joint2, joint3: Joint angles in radians as defined in the URDF file
    """
    # Calculate joint2 and joint3 offsets in theta1 and theta2
    theta1_offset = math.atan2(0.028, 0.11257)  # theta1 offset when joint2=0
    theta2_offset = math.atan2(0.0052, 0.1349) + theta1_offset  # theta2 offset when joint3=0
    
    # Calculate distance from origin to target point
    r = math.sqrt(x**2 + y**2)
    r_max = l1 + l2  # Maximum reachable distance
    
    # If target point is beyond maximum workspace, scale it to the boundary
    if r > r_max:
        scale_factor = r_max / r
        x *= scale_factor
        y *= scale_factor
        r = r_max
    
    # If target point is less than minimum workspace (|l1-l2|), scale it
    r_min = abs(l1 - l2)
    if r < r_min and r > 0:
        scale_factor = r_min / r
        x *= scale_factor
        y *= scale_factor
        r = r_min
    
    # Use law of cosines to calculate theta2
    cos_theta2 = -(r**2 - l1**2 - l2**2) / (2 * l1 * l2)
    
    # Calculate theta2 (elbow angle)
    theta2 = math.pi - math.acos(cos_theta2)
    
    # Calculate theta1 (shoulder angle)
    beta = math.atan2(y, x)
    gamma = math.atan2(l2 * math.sin(theta2), l1 + l2 * math.cos(theta2))
    theta1 = beta + gamma
    
    # Convert theta1 and theta2 to joint2 and joint3 angles
    joint2 = theta1 + theta1_offset
    joint3 = theta2 + theta2_offset
    
    # Ensure angles are within URDF limits
    joint2 = max(-0.1, min(3.45, joint2))
    joint3 = max(-0.2, min(math.pi, joint3))
    
    # Convert from radians to degrees
    joint2_deg = math.degrees(joint2)
    joint3_deg = math.degrees(joint3)

    joint2_deg = 90-joint2_deg
    joint3_deg = joint3_deg-90
    
    return joint2_deg, joint3_deg

def move_to_zero_position(robot, duration=3.0, kp=0.5):
    """
    ä½¿ç”¨Pæ§åˆ¶ç¼“æ…¢ç§»åŠ¨æœºå™¨äººåˆ°é›¶ä½ç½®
    
    Args:
        robot: æœºå™¨äººå®ä¾‹
        duration: ç§»åŠ¨åˆ°é›¶ä½ç½®æ‰€éœ€æ—¶é—´(ç§’)
        kp: æ¯”ä¾‹å¢ç›Š
    """
    print("æ­£åœ¨ä½¿ç”¨Pæ§åˆ¶ç¼“æ…¢ç§»åŠ¨æœºå™¨äººåˆ°é›¶ä½ç½®...")
    
    # è·å–å½“å‰æœºå™¨äººçŠ¶æ€
    current_obs = robot.get_observation()
    
    # æå–å½“å‰å…³èŠ‚ä½ç½®
    current_positions = {}
    for key, value in current_obs.items():
        if key.endswith('.pos'):
            motor_name = key.removesuffix('.pos')
            current_positions[motor_name] = value
    
    # é›¶ä½ç½®ç›®æ ‡
    zero_positions = {
        'shoulder_pan': 0.0,
        'shoulder_lift': 0.0,
        'elbow_flex': 0.0,
        'wrist_flex': 0.0,
        'wrist_roll': 0.0,
        'gripper': 0.0
    }
    
    # è®¡ç®—æ§åˆ¶æ­¥æ•°
    control_freq = 60  # 50Hzæ§åˆ¶é¢‘ç‡
    total_steps = int(duration * control_freq)
    step_time = 1.0 / control_freq
    
    print(f"å°†åœ¨ {duration} ç§’å†…ä½¿ç”¨Pæ§åˆ¶ç§»åŠ¨åˆ°é›¶ä½ç½®ï¼Œæ§åˆ¶é¢‘ç‡: {control_freq}Hzï¼Œæ¯”ä¾‹å¢ç›Š: {kp}")
    
    for step in range(total_steps):
        # è·å–å½“å‰æœºå™¨äººçŠ¶æ€
        current_obs = robot.get_observation()
        current_positions = {}
        for key, value in current_obs.items():
            if key.endswith('.pos'):
                motor_name = key.removesuffix('.pos')
                # åº”ç”¨æ ¡å‡†ç³»æ•°
                calibrated_value = apply_joint_calibration(motor_name, value)
                current_positions[motor_name] = calibrated_value
        
        # Pæ§åˆ¶è®¡ç®—
        robot_action = {}
        for joint_name, target_pos in zero_positions.items():
            if joint_name in current_positions:
                current_pos = current_positions[joint_name]
                error = target_pos - current_pos
                
                # Pæ§åˆ¶: è¾“å‡º = Kp * è¯¯å·®
                control_output = kp * error
                
                # å°†æ§åˆ¶è¾“å‡ºè½¬æ¢ä¸ºä½ç½®å‘½ä»¤
                new_position = current_pos + control_output
                robot_action[f"{joint_name}.pos"] = new_position
        
        # å‘é€åŠ¨ä½œåˆ°æœºå™¨äºº
        if robot_action:
            robot.send_action(robot_action)
        
        # æ˜¾ç¤ºè¿›åº¦
        if step % (control_freq // 2) == 0:  # æ¯0.5ç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            progress = (step / total_steps) * 100
            print(f"ç§»åŠ¨åˆ°é›¶ä½ç½®è¿›åº¦: {progress:.1f}%")
        
        time.sleep(step_time)
    
    print("æœºå™¨äººå·²ç§»åŠ¨åˆ°é›¶ä½ç½®")

def return_to_start_position(robot, start_positions, kp=0.5, control_freq=50):
    """
    ä½¿ç”¨Pæ§åˆ¶è¿”å›åˆ°èµ·å§‹ä½ç½®
    
    Args:
        robot: æœºå™¨äººå®ä¾‹
        start_positions: èµ·å§‹å…³èŠ‚ä½ç½®å­—å…¸
        kp: æ¯”ä¾‹å¢ç›Š
        control_freq: æ§åˆ¶é¢‘ç‡(Hz)
    """
    print("æ­£åœ¨è¿”å›åˆ°èµ·å§‹ä½ç½®...")
    
    control_period = 1.0 / control_freq
    max_steps = int(5.0 * control_freq)  # æœ€å¤š5ç§’
    
    for step in range(max_steps):
        # è·å–å½“å‰æœºå™¨äººçŠ¶æ€
        current_obs = robot.get_observation()
        current_positions = {}
        for key, value in current_obs.items():
            if key.endswith('.pos'):
                motor_name = key.removesuffix('.pos')
                current_positions[motor_name] = value  # ä¸åº”ç”¨æ ¡å‡†ç³»æ•°
        
        # Pæ§åˆ¶è®¡ç®—
        robot_action = {}
        total_error = 0
        for joint_name, target_pos in start_positions.items():
            if joint_name in current_positions:
                current_pos = current_positions[joint_name]
                error = target_pos - current_pos
                total_error += abs(error)
                
                # Pæ§åˆ¶: è¾“å‡º = Kp * è¯¯å·®
                control_output = kp * error
                
                # å°†æ§åˆ¶è¾“å‡ºè½¬æ¢ä¸ºä½ç½®å‘½ä»¤
                new_position = current_pos + control_output
                robot_action[f"{joint_name}.pos"] = new_position
        
        # å‘é€åŠ¨ä½œåˆ°æœºå™¨äºº
        if robot_action:
            robot.send_action(robot_action)
        
        # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾èµ·å§‹ä½ç½®
        if total_error < 2.0:  # å¦‚æœæ€»è¯¯å·®å°äº2åº¦ï¼Œè®¤ä¸ºå·²åˆ°è¾¾
            print("å·²è¿”å›åˆ°èµ·å§‹ä½ç½®")
            break
        
        time.sleep(control_period)
    
    print("è¿”å›èµ·å§‹ä½ç½®å®Œæˆ")

# Mapping coefficients for vision control
K_pan = -0.006  # radians per pixel (tune as needed)
K_y = 0.00008   # meters per pixel (tune as needed)

# Global trigger flag
trigger_received = False

# use for test
def check_trigger(ser):
    global trigger_received
    try:
        if ser.in_waiting > 0:
            data = ser.readline().decode('utf-8').strip()
            if data == 'true':
                trigger_received = True
                print("æ”¶åˆ°è§¦å‘ä¿¡å·: true")
    except Exception as e:
        print(f"ä¸²å£è¯»å–é”™è¯¯: {e}")

# Vision control update function
def vision_control_update(target_positions, current_x, current_y, model, cap, K_pan, K_y):
    ret, frame = cap.read()
    if not ret:
        print("Camera frame not available")
        return current_x, current_y  # No update

    results = model(frame)
    if not results or not hasattr(results[0], 'boxes') or not results[0].boxes:
        print("No objects detected")
        annotated_frame = frame
    else:
        # Find "bottle" in detections
        annotated_frame = results[0].plot()
        for box in results[0].boxes:
            cls = int(box.cls[0])
            label = results[0].names[cls]
            if label == "bottle":
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                cx = (x1 + x2) // 2
                cy = (y1 + y2) // 2
                h, w = frame.shape[:2]
                dx = cx - w // 2
                dy = cy - h // 2
                # Map dx, dy to robot control
                d_current_x = -K_pan * dx
                if abs(d_current_x) < 0.1:
                    d_current_x = 0

                target_positions['shoulder_pan'] += d_current_x
                d_current_y = -K_y * dy 
                # Clamp d_current_y to [-0.004, 0.004] and zero if within Â±0.0005
                d_current_y = max(min(d_current_y, 0.005), -0.005)
                if abs(d_current_y) < 0.002:
                    d_current_y = 0
                current_y += d_current_y   # Negative sign: up in image = increase y
                # Update joint targets using inverse kinematics
                joint2_target, joint3_target = inverse_kinematics(current_x, current_y)
                target_positions['shoulder_lift'] = joint2_target
                target_positions['elbow_flex'] = joint3_target
                print(f"Bottle center offset: dx={dx}, dy={dy} -> dc_y={d_current_y}, pan: {target_positions['shoulder_pan']:.2f}, y: {current_y:.3f}, joint2: {joint2_target:.2f}, joint3: {joint3_target:.2f}")
                break  # Only use first bottle found
    # Show annotated frame in a window
    cv2.imshow("YOLO11 Live", annotated_frame)
    # Allow quitting vision mode with 'q'
    if cv2.waitKey(1) & 0xFF == ord("q"):
        raise KeyboardInterrupt
    return current_x, current_y

def p_control_loop(robot, keyboard, target_positions, start_positions, current_x, current_y, kp=0.5, control_freq=50, model=None, cap=None, vision_mode=False, ser=None):
    """
    Pæ§åˆ¶å¾ªç¯
    
    Args:
        robot: æœºå™¨äººå®ä¾‹
        keyboard: é”®ç›˜å®ä¾‹
        target_positions: ç›®æ ‡å…³èŠ‚ä½ç½®å­—å…¸
        start_positions: èµ·å§‹å…³èŠ‚ä½ç½®å­—å…¸
        current_x: å½“å‰xåæ ‡
        current_y: å½“å‰yåæ ‡
        kp: æ¯”ä¾‹å¢ç›Š
        control_freq: æ§åˆ¶é¢‘ç‡(Hz)
        model: YOLOæ¨¡å‹å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        cap: æ‘„åƒå¤´å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        vision_mode: æ˜¯å¦å¯ç”¨è§†è§‰æ§åˆ¶
    """
    control_period = 1.0 / control_freq
    
    # åˆå§‹åŒ–pitchæ§åˆ¶å˜é‡
    pitch = 0.0  # åˆå§‹pitchè°ƒæ•´
    pitch_step = 1  # pitchè°ƒæ•´æ­¥é•¿
    
    print(f"å¼€å§‹Pæ§åˆ¶å¾ªç¯ï¼Œæ§åˆ¶é¢‘ç‡: {control_freq}Hzï¼Œæ¯”ä¾‹å¢ç›Š: {kp}")
    
    while True:
        try:
            # æ£€æŸ¥triggerä¿¡å·
            if ser is not None:
                check_trigger(ser)
                global trigger_received
                if trigger_received:
                    print("trigger")
                    # for trigger test
                    target_positions['shoulder_pan'] = 30.0
                    target_positions['shoulder_lift'] = 20.0
                    target_positions['elbow_flex'] = -30.0
                    target_positions['wrist_flex'] = 10.0
                    target_positions['wrist_roll'] = 0.0
                    target_positions['gripper'] = 0.0
                    print("done")
                    # é‡ç½®triggeræ ‡å¿—
                    trigger_received = False
            
            if vision_mode and model is not None and cap is not None:
                # Vision-based control
                current_x, current_y = vision_control_update(
                    target_positions, current_x, current_y, model, cap, K_pan, K_y
                )
            else:
                # è·å–é”®ç›˜è¾“å…¥
                keyboard_action = keyboard.get_action()
                
                if keyboard_action:
                    # å¤„ç†é”®ç›˜è¾“å…¥ï¼Œæ›´æ–°ç›®æ ‡ä½ç½®
                    for key, value in keyboard_action.items():
                        if key == 'x':
                            # é€€å‡ºç¨‹åºï¼Œå…ˆå›åˆ°èµ·å§‹ä½ç½®
                            print("æ£€æµ‹åˆ°é€€å‡ºå‘½ä»¤ï¼Œæ­£åœ¨å›åˆ°èµ·å§‹ä½ç½®...")
                            return_to_start_position(robot, start_positions, 0.2, control_freq)
                            return
                        
                        # å…³èŠ‚æ§åˆ¶æ˜ å°„
                        joint_controls = {
                            'q': ('shoulder_pan', -1),    # å…³èŠ‚1å‡å°‘
                            'a': ('shoulder_pan', 1),     # å…³èŠ‚1å¢åŠ 
                            't': ('wrist_roll', -1),      # å…³èŠ‚5å‡å°‘
                            'g': ('wrist_roll', 1),       # å…³èŠ‚5å¢åŠ 
                            'y': ('gripper', -1),         # å…³èŠ‚6å‡å°‘
                            'h': ('gripper', 1),          # å…³èŠ‚6å¢åŠ 
                        }
                        
                        # x,yåæ ‡æ§åˆ¶
                        xy_controls = {
                            'w': ('x', -0.004),  # xå‡å°‘
                            's': ('x', 0.004),   # xå¢åŠ 
                            'e': ('y', -0.004),  # yå‡å°‘
                            'd': ('y', 0.004),   # yå¢åŠ 
                        }
                        
                        # pitchæ§åˆ¶
                        if key == 'r':
                            pitch += pitch_step
                            print(f"å¢åŠ pitchè°ƒæ•´: {pitch:.3f}")
                        elif key == 'f':
                            pitch -= pitch_step
                            print(f"å‡å°‘pitchè°ƒæ•´: {pitch:.3f}")
                        
                        if key in joint_controls:
                            joint_name, delta = joint_controls[key]
                            if joint_name in target_positions:
                                current_target = target_positions[joint_name]
                                new_target = int(current_target + delta)
                                target_positions[joint_name] = new_target
                                print(f"æ›´æ–°ç›®æ ‡ä½ç½® {joint_name}: {current_target} -> {new_target}")
                        
                        elif key in xy_controls:
                            coord, delta = xy_controls[key]
                            if coord == 'x':
                                current_x += delta
                                # è®¡ç®—joint2å’Œjoint3çš„ç›®æ ‡è§’åº¦
                                joint2_target, joint3_target = inverse_kinematics(current_x, current_y)
                                target_positions['shoulder_lift'] = joint2_target
                                target_positions['elbow_flex'] = joint3_target
                                print(f"æ›´æ–°xåæ ‡: {current_x:.4f}, joint2={joint2_target:.3f}, joint3={joint3_target:.3f}")
                            elif coord == 'y':
                                current_y += delta
                                # è®¡ç®—joint2å’Œjoint3çš„ç›®æ ‡è§’åº¦
                                joint2_target, joint3_target = inverse_kinematics(current_x, current_y)
                                target_positions['shoulder_lift'] = joint2_target
                                target_positions['elbow_flex'] = joint3_target
                                print(f"æ›´æ–°yåæ ‡: {current_y:.4f}, joint2={joint2_target:.3f}, joint3={joint3_target:.3f}")
            
            # åº”ç”¨pitchè°ƒæ•´åˆ°wrist_flex
            # åŸºäºshoulder_liftå’Œelbow_flexè®¡ç®—wrist_flexçš„ç›®æ ‡ä½ç½®
            if 'shoulder_lift' in target_positions and 'elbow_flex' in target_positions:
                target_positions['wrist_flex'] = - target_positions['shoulder_lift'] - target_positions['elbow_flex'] + pitch
                # æ˜¾ç¤ºå½“å‰pitchå€¼ï¼ˆæ¯100æ­¥æ˜¾ç¤ºä¸€æ¬¡ï¼Œé¿å…åˆ·å±ï¼‰
                if hasattr(p_control_loop, 'step_counter'):
                    p_control_loop.step_counter += 1
                else:
                    p_control_loop.step_counter = 0
                
                if p_control_loop.step_counter % 100 == 0:
                    print(f"å½“å‰pitchè°ƒæ•´: {pitch:.3f}, wrist_flexç›®æ ‡: {target_positions['wrist_flex']:.3f}")
            
            # è·å–å½“å‰æœºå™¨äººçŠ¶æ€
            current_obs = robot.get_observation()
            
            # æå–å½“å‰å…³èŠ‚ä½ç½®
            current_positions = {}
            for key, value in current_obs.items():
                if key.endswith('.pos'):
                    motor_name = key.removesuffix('.pos')
                    # åº”ç”¨æ ¡å‡†ç³»æ•°
                    calibrated_value = apply_joint_calibration(motor_name, value)
                    current_positions[motor_name] = calibrated_value
            
            # Pæ§åˆ¶è®¡ç®—
            robot_action = {}
            for joint_name, target_pos in target_positions.items():
                if joint_name in current_positions:
                    current_pos = current_positions[joint_name]
                    error = target_pos - current_pos
                    
                    # Pæ§åˆ¶: è¾“å‡º = Kp * è¯¯å·®
                    control_output = kp * error
                    
                    # å°†æ§åˆ¶è¾“å‡ºè½¬æ¢ä¸ºä½ç½®å‘½ä»¤
                    new_position = current_pos + control_output
                    robot_action[f"{joint_name}.pos"] = new_position
            
            # å‘é€åŠ¨ä½œåˆ°æœºå™¨äºº
            if robot_action:
                robot.send_action(robot_action)
            
            time.sleep(control_period)
            
        except KeyboardInterrupt:
            print("ç”¨æˆ·ä¸­æ–­ç¨‹åº")
            break
        except Exception as e:
            print(f"Pæ§åˆ¶å¾ªç¯å‡ºé”™: {e}")
            traceback.print_exc()
            break

def main():
    """ä¸»å‡½æ•°"""
    print("LeRobot ç®€åŒ–é”®ç›˜æ§åˆ¶ç¤ºä¾‹ (Pæ§åˆ¶)")
    print("="*50)
    
    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from lerobot.robots.so100_follower import SO100Follower, SO100FollowerConfig
        from lerobot.teleoperators.keyboard import KeyboardTeleop, KeyboardTeleopConfig
        
        # è·å–ç«¯å£
        port = input("è¯·è¾“å…¥SO100æœºå™¨äººçš„USBç«¯å£ (ä¾‹å¦‚: /dev/ttyACM0): ").strip()
        
        # å¦‚æœç›´æ¥æŒ‰å›è½¦ï¼Œä½¿ç”¨é»˜è®¤ç«¯å£
        if not port:
            port = "/dev/ttyACM0"
            print(f"ä½¿ç”¨é»˜è®¤ç«¯å£: {port}")
        else:
            print(f"è¿æ¥åˆ°ç«¯å£: {port}")
        
        # é…ç½®æœºå™¨äºº
        robot_config = SO100FollowerConfig(port=port)
        robot = SO100Follower(robot_config)
        
        # é…ç½®é”®ç›˜
        keyboard_config = KeyboardTeleopConfig()
        keyboard = KeyboardTeleop(keyboard_config)
        
        # åˆå§‹åŒ–ä¸²å£ç”¨äºtrigger
        try:
            ser = serial.Serial('/dev/ttyACM0', 115200, timeout=0.1)
            print("ä¸²å£è¿æ¥æˆåŠŸï¼Œç­‰å¾…triggerä¿¡å·...")
        except Exception as e:
            print(f"ä¸²å£è¿æ¥å¤±è´¥: {e}")
            ser = None
        
        # è¿æ¥è®¾å¤‡
        robot.connect()
        keyboard.connect()
        
        print("è®¾å¤‡è¿æ¥æˆåŠŸï¼")
        
        # è¯¢é—®æ˜¯å¦é‡æ–°æ ¡å‡†
        while True:
            calibrate_choice = input("æ˜¯å¦é‡æ–°æ ¡å‡†æœºå™¨äºº? (y/n): ").strip().lower()
            if calibrate_choice in ['y', 'yes', 'æ˜¯']:
                print("å¼€å§‹é‡æ–°æ ¡å‡†...")
                robot.calibrate()
                print("æ ¡å‡†å®Œæˆï¼")
                break
            elif calibrate_choice in ['n', 'no', 'å¦']:
                print("ä½¿ç”¨ä¹‹å‰çš„æ ¡å‡†æ–‡ä»¶")
                break
            else:
                print("è¯·è¾“å…¥ y æˆ– n")
        
        # è¯»å–èµ·å§‹å…³èŠ‚è§’åº¦
        print("è¯»å–èµ·å§‹å…³èŠ‚è§’åº¦...")
        start_obs = robot.get_observation()
        start_positions = {}
        for key, value in start_obs.items():
            if key.endswith('.pos'):
                motor_name = key.removesuffix('.pos')
                start_positions[motor_name] = int(value)  # ä¸åº”ç”¨æ ¡å‡†ç³»æ•°
        
        print("èµ·å§‹å…³èŠ‚è§’åº¦:")
        for joint_name, position in start_positions.items():
            print(f"  {joint_name}: {position}Â°")
        
        # ç§»åŠ¨åˆ°é›¶ä½ç½®
        move_to_zero_position(robot, duration=3.0)
        
        # åˆå§‹åŒ–ç›®æ ‡ä½ç½®ä¸ºå½“å‰ä½ç½®ï¼ˆæ•´æ•°ï¼‰
        target_positions = {
        'shoulder_pan': 0.0,
        'shoulder_lift': 0.0,
        'elbow_flex': 0.0,
        'wrist_flex': 0.0,
        'wrist_roll': 0.0,
        'gripper': 0.0
          }
        
        # åˆå§‹åŒ–x,yåæ ‡æ§åˆ¶
        x0, y0 = 0.1629, 0.1131
        current_x, current_y = x0, y0
        print(f"åˆå§‹åŒ–æœ«ç«¯æ‰§è¡Œå™¨ä½ç½®: x={current_x:.4f}, y={current_y:.4f}")
        
        # Initialize YOLO and camera
        model = YOLO("yolov8n.pt")
        
        # Direct camera selection (fastest approach)
        camera_input = input("Enter camera ID (0/1/2) or press Enter for camera 0: ").strip()
        selected = int(camera_input) if camera_input.isdigit() else 0
        
        print(f"ğŸ“· Connecting to camera {selected}...")
        cap = cv2.VideoCapture(selected)
        
        if cap.isOpened():
            # Quick test read
            ret, _ = cap.read()
            if ret:
                # Set camera properties
                cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
                cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                print(f"âœ… Camera {selected} ready!")
                vision_mode = True
            else:
                print(f"âŒ Camera {selected} cannot read frames")
                cap.release()
                cap = None
                vision_mode = False
        else:
            print(f"âŒ Camera {selected} not available")
            cap = None
            vision_mode = False
        
        if not vision_mode:
            print("âš ï¸  Continuing in keyboard-only mode")
            model = None

        print("é”®ç›˜æ§åˆ¶è¯´æ˜:")
        print("- Q/A: å…³èŠ‚1 (shoulder_pan) å‡å°‘/å¢åŠ ")
        print("- W/S: æ§åˆ¶æœ«ç«¯æ‰§è¡Œå™¨xåæ ‡ (joint2+3)")
        print("- E/D: æ§åˆ¶æœ«ç«¯æ‰§è¡Œå™¨yåæ ‡ (joint2+3)")
        print("- R/F: pitchè°ƒæ•´ å¢åŠ /å‡å°‘ (å½±å“wrist_flex)")
        print("- T/G: å…³èŠ‚5 (wrist_roll) å‡å°‘/å¢åŠ ")
        print("- Y/H: å…³èŠ‚6 (gripper) å‡å°‘/å¢åŠ ")
        print("- X: é€€å‡ºç¨‹åºï¼ˆå…ˆå›åˆ°èµ·å§‹ä½ç½®ï¼‰")
        print("- ESC: é€€å‡ºç¨‹åº")
        print("="*50)
        print("æ³¨æ„: æœºå™¨äººä¼šæŒç»­ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®")
        
        # å¯ç”¨è§†è§‰æ§åˆ¶
        vision_mode = True
        p_control_loop(robot, keyboard, target_positions, start_positions, current_x, current_y, kp=0.5, control_freq=50, model=model, cap=cap, vision_mode=vision_mode, ser=ser)
        
        # æ–­å¼€è¿æ¥
        robot.disconnect()
        keyboard.disconnect()
        if ser is not None:
            ser.close()
        cap.release()
        cv2.destroyAllWindows()
        print("ç¨‹åºç»“æŸ")
        
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        traceback.print_exc()
        print("è¯·æ£€æŸ¥:")
        print("1. æœºå™¨äººæ˜¯å¦æ­£ç¡®è¿æ¥")
        print("2. USBç«¯å£æ˜¯å¦æ­£ç¡®")
        print("3. æ˜¯å¦æœ‰è¶³å¤Ÿçš„æƒé™è®¿é—®USBè®¾å¤‡")
        print("4. æœºå™¨äººæ˜¯å¦å·²æ­£ç¡®é…ç½®")

if __name__ == "__main__":
    main()
